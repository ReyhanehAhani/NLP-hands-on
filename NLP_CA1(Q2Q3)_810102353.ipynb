{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2_3\n",
        "Implemeting tokenizer by BPE and WordPiece"
      ],
      "metadata": {
        "id": "v3191W1qsgrJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkURsJxUr9_i",
        "outputId": "56d108b0-db68-428e-c5fb-2723126309ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', ' ', 'project', ' ', 'gutenberg', ' ', 'ebook', ' ', 'of', ' ', 'all', ' ', 'around', ' ', 'the', ' ', 'moon', ' ', 'this', ' ', 'ebook', ' ', 'is', ' ', 'for', ' ', 'the', ' ', 'use', ' ', 'of', ' ', 'anyone', ' ', 'anywhere', ' ', 'in', ' ', 'the', ' ', 'united', ' ', 'states', ' ', 'and', ' ', 'most', ' ', 'other', ' ', 'parts', ' ', 'of', ' ', 'the', ' ', 'world', ' ', 'at', ' ', 'no', ' ', 'cost', ' ', 'and', ' ', 'with', ' ', 'almost', ' ', 'no', ' ', 'restrictions', ' ', 'whatsoever', ' ', 'you', ' ', 'may', ' ', 'copy', ' ', 'it', ' ', 'give', ' ', 'it', ' ', 'away', ' ', 'or', ' ', 'reuse', ' ', 'it', ' ', 'under', ' ', 'the', ' ']\n"
          ]
        }
      ],
      "source": [
        "#wordpiece\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "class WordPieceTokenizer:\n",
        "    def __init__(self, vocab, max_word_length=20):\n",
        "        self.vocab = vocab\n",
        "        self.max_word_length = max_word_length\n",
        "\n",
        "    def _build_subwords(self):\n",
        "        subwords = set()\n",
        "        for word in self.vocab:\n",
        "            for i in range(len(word)):\n",
        "                for j in range(i + 1, min(len(word), i + self.max_word_length) + 1):\n",
        "                    subwords.add(word[i:j])\n",
        "        return subwords\n",
        "\n",
        "    def _train_wordpiece(self, num_iterations=10):\n",
        "        for _ in range(num_iterations):\n",
        "            subwords = self._build_subwords()\n",
        "            for subword in subwords:\n",
        "                if subword not in self.vocab:\n",
        "                    self.vocab[subword] = len(self.vocab)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = []\n",
        "        start = 0\n",
        "        while start < len(text):\n",
        "            match = False\n",
        "            for end in range(min(len(text), start + self.max_word_length), start, -1):\n",
        "                substr = text[start:end]\n",
        "                if substr in self.vocab:\n",
        "                    tokens.append(substr)\n",
        "                    start = end\n",
        "                    match = True\n",
        "                    break\n",
        "            if not match:\n",
        "                tokens.append(text[start])\n",
        "                start += 1\n",
        "        return tokens\n",
        "\n",
        "def preprocess_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    words = text.split()\n",
        "    return words\n",
        "\n",
        "\n",
        "file_path = '/content/All_Around_the_Moon.txt'\n",
        "\n",
        "words = preprocess_text(file_path)\n",
        "vocab = {word: i for i, word in enumerate(Counter(words))}\n",
        "tokenizer = WordPieceTokenizer(vocab)\n",
        "tokenizer._train_wordpiece()\n",
        "all_tokens = tokenizer.tokenize(' '.join(words))\n",
        "print(all_tokens[:100])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BPE\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "class BPE:\n",
        "    def __init__(self, corpus, vocab_size):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = self.train(corpus)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        vocab = {(' '.join(word) + ' </w>',): freq for word, freq in corpus.items()}\n",
        "\n",
        "        while len(self.flatten_vocab(vocab)) < self.vocab_size:\n",
        "            pairs = self.get_stats(vocab)\n",
        "            if not pairs:\n",
        "                break\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            vocab = self.merge_best_pair(best_pair, vocab)\n",
        "        return self.flatten_vocab(vocab)\n",
        "\n",
        "    def flatten_vocab(self, vocab):\n",
        "        return set([subword for word in vocab for subword in word])\n",
        "\n",
        "    def get_stats(self, vocab):\n",
        "        pairs = Counter()\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word[0].split()\n",
        "            for i in range(len(symbols)-1):\n",
        "                pairs[symbols[i], symbols[i+1]] += freq\n",
        "        return pairs\n",
        "\n",
        "    def merge_best_pair(self, best_pair, vocab):\n",
        "        new_vocab = {}\n",
        "        bigram = ' '.join(best_pair)\n",
        "        replacer = ''.join(best_pair)\n",
        "        for word in vocab:\n",
        "            word_str = word[0]\n",
        "            new_word = word_str.replace(bigram, replacer)\n",
        "            new_vocab[(new_word,)] = vocab[word]\n",
        "        return new_vocab\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-z\\s]', '', text)\n",
        "        words = text.split()\n",
        "        tokens = []\n",
        "        for word in words:\n",
        "            word = ' '.join(word) + ' </w>'\n",
        "            word_tokens = []\n",
        "            while word:\n",
        "                possible_tokens = [token for token in self.vocab if word.startswith(token.replace('</w>', ' </w>'))]\n",
        "                if not possible_tokens:\n",
        "                    word_tokens.append(word)\n",
        "                    break\n",
        "                longest_token = max(possible_tokens, key=len)\n",
        "                word_tokens.append(longest_token)\n",
        "                word = word[len(longest_token):].lstrip()\n",
        "            tokens.extend(word_tokens)\n",
        "        return tokens\n",
        "\n",
        "file_path = '/content/All_Around_the_Moon.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "processed_text = re.sub(r'[^a-z\\s]', '', text_data.lower())\n",
        "words = processed_text.split()\n",
        "word_freq = Counter(words)\n",
        "bpe_tokenizer = BPE(word_freq, vocab_size=50000)\n",
        "tokenized_text = bpe_tokenizer.tokenize(processed_text)\n",
        "print(tokenized_text[:100])"
      ],
      "metadata": {
        "id": "geqaQ6nwt1d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5960b4e-2dbc-4182-980e-236670aec91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['t h e </w>', 'p r o j e c t </w>', 'g u t e n b e r g </w>', 'e b o o k </w>', 'o f </w>', 'a l l </w>', 'a r o u n d </w>', 't h e </w>', 'm o o n </w>', 't h i s </w>', 'e b o o k </w>', 'i s </w>', 'f o r </w>', 't h e </w>', 'u s e </w>', 'o f </w>', 'a n y o n e </w>', 'a n y w h e r e </w>', 'i n </w>', 't h e </w>', 'u n i t e d </w>', 's t a t e s </w>', 'a n d </w>', 'm o s t </w>', 'o t h e r </w>', 'p a r t s </w>', 'o f </w>', 't h e </w>', 'w o r l d </w>', 'a t </w>', 'n o </w>', 'c o s t </w>', 'a n d </w>', 'w i t h </w>', 'a l m o s t </w>', 'n o </w>', 'r e s t r i c t i o n s </w>', 'w h a t s o e v e r </w>', 'y o u </w>', 'm a y </w>', 'c o p y </w>', 'i t </w>', 'g i v e </w>', 'i t </w>', 'a w a y </w>', 'o r </w>', 'r e u s e </w>', 'i t </w>', 'u n d e r </w>', 't h e </w>', 't e r m s </w>', 'o f </w>', 't h e </w>', 'p r o j e c t </w>', 'g u t e n b e r g </w>', 'l i c e n s e </w>', 'i n c l u d e d </w>', 'w i t h </w>', 't h i s </w>', 'e b o o k </w>', 'o r </w>', 'o n l i n e </w>', 'a t </w>', 'w w w g u t e n b e r g o r g </w>', 'i f </w>', 'y o u </w>', 'a r e </w>', 'n o t </w>', 'l o c a t e d </w>', 'i n </w>', 't h e </w>', 'u n i t e d </w>', 's t a t e s </w>', 'y o u </w>', 'w i l l </w>', 'h a v e </w>', 't o </w>', 'c h e c k </w>', 't h e </w>', 'l a w s </w>', 'o f </w>', 't h e </w>', 'c o u n t r y </w>', 'w h e r e </w>', 'y o u </w>', 'a r e </w>', 'l o c a t e d </w>', 'b e f o r e </w>', 'u s i n g </w>', 't h i s </w>', 'e b o o k </w>', 't i t l e </w>', 'a l l </w>', 'a r o u n d </w>', 't h e </w>', 'm o o n </w>', 'a u t h o r </w>', 'j u l e s </w>', 'v e r n e </w>', 't r a n s l a t o r </w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokeninzing the following sentences:\n",
        "\n",
        "- This darkness is absolutely killing! If we ever take this trip again, it must be about the time of the sNew Moon!\n",
        "\n",
        "- This is a tokenization task. Tokenization is the first step in a NLP pipeline. We will be comparing the tokens generated by each tokenization model."
      ],
      "metadata": {
        "id": "qu6XPPDlJ_Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Wordpiece\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "class WordPieceTokenizer:\n",
        "    def __init__(self, vocab, max_word_length=20):\n",
        "        self.vocab = vocab\n",
        "        self.max_word_length = max_word_length\n",
        "\n",
        "    def _build_subwords(self):\n",
        "        subwords = set()\n",
        "        for word in self.vocab:\n",
        "            for i in range(len(word)):\n",
        "                for j in range(i + 1, min(len(word), i + self.max_word_length) + 1):\n",
        "                    subwords.add(word[i:j])\n",
        "        return subwords\n",
        "\n",
        "    def _train_wordpiece(self, num_iterations=10):\n",
        "        for _ in range(num_iterations):\n",
        "            subwords = self._build_subwords()\n",
        "            for subword in subwords:\n",
        "                if subword not in self.vocab:\n",
        "                    self.vocab[subword] = len(self.vocab)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = []\n",
        "        start = 0\n",
        "        while start < len(text):\n",
        "            match = False\n",
        "            for end in range(min(len(text), start + self.max_word_length), start, -1):\n",
        "                substr = text[start:end]\n",
        "                if substr in self.vocab:\n",
        "                    tokens.append(substr)\n",
        "                    start = end\n",
        "                    match = True\n",
        "                    break\n",
        "            if not match:\n",
        "                tokens.append(text[start])\n",
        "                start += 1\n",
        "        return tokens\n",
        "\n",
        "def preprocess_sentences(sentences):\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
        "        processed_sentences.extend(sentence.split())\n",
        "    return processed_sentences\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    \"This darkness is absolutely killing! If we ever take this trip again, it must be about the time of the sNew Moon!\",\n",
        "    \"This is a tokenization task. Tokenization is the first step in a NLP pipeline. We will be comparing the tokens generated by each tokenization model.\"\n",
        "]\n",
        "\n",
        "words = preprocess_sentences(sentences)\n",
        "vocab = {word: i for i, word in enumerate(Counter(words))}\n",
        "\n",
        "tokenizer = WordPieceTokenizer(vocab)\n",
        "tokenizer._train_wordpiece()\n",
        "\n",
        "all_tokens = []\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    all_tokens.extend(tokens)\n",
        "print(all_tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cq4kcUrJ-Oh",
        "outputId": "f0ad7c10-cc7a-4c55-ab87-28c4e59d66be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T', 'his', ' ', 'darkness', ' ', 'is', ' ', 'absolutely', ' ', 'killing', '!', ' ', 'I', 'f', ' ', 'we', ' ', 'ever', ' ', 'take', ' ', 'this', ' ', 'trip', ' ', 'again', ',', ' ', 'it', ' ', 'must', ' ', 'be', ' ', 'about', ' ', 'the', ' ', 'time', ' ', 'of', ' ', 'the', ' ', 's', 'N', 'ew', ' ', 'M', 'oon', '!', 'T', 'his', ' ', 'is', ' ', 'a', ' ', 'tokenization', ' ', 'task', '.', ' ', 'T', 'okenization', ' ', 'is', ' ', 'the', ' ', 'first', ' ', 'step', ' ', 'in', ' ', 'a', ' ', 'N', 'L', 'P', ' ', 'pipeline', '.', ' ', 'W', 'e', ' ', 'will', ' ', 'be', ' ', 'comparing', ' ', 'the', ' ', 'tokens', ' ', 'generated', ' ', 'by', ' ', 'each', ' ', 'tokenization', ' ', 'model', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BPE\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "class BPE:\n",
        "    def __init__(self, corpus, vocab_size):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = self.train(corpus)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        vocab = {(' '.join(word) + ' </w>',): freq for word, freq in corpus.items()}\n",
        "        while len(self.flatten_vocab(vocab)) < self.vocab_size:\n",
        "            pairs = self.get_stats(vocab)\n",
        "            if not pairs:\n",
        "                break\n",
        "            best_pair = max(pairs, key=pairs.get)\n",
        "            vocab = self.merge_best_pair(best_pair, vocab)\n",
        "        return self.flatten_vocab(vocab)\n",
        "\n",
        "    def flatten_vocab(self, vocab):\n",
        "        return set([subword for word in vocab for subword in word])\n",
        "\n",
        "    def get_stats(self, vocab):\n",
        "        pairs = Counter()\n",
        "        for word, freq in vocab.items():\n",
        "            symbols = word[0].split()\n",
        "            for i in range(len(symbols)-1):\n",
        "                pairs[symbols[i], symbols[i+1]] += freq\n",
        "        return pairs\n",
        "\n",
        "    def merge_best_pair(self, best_pair, vocab):\n",
        "        new_vocab = {}\n",
        "        bigram = ' '.join(best_pair)\n",
        "        replacer = ''.join(best_pair)\n",
        "        for word in vocab:\n",
        "            word_str = word[0]\n",
        "            new_word = word_str.replace(bigram, replacer)\n",
        "            new_vocab[(new_word,)] = vocab[word]\n",
        "        return new_vocab\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-z\\s]', '', text)\n",
        "        words = text.split()\n",
        "        tokens = []\n",
        "        for word in words:\n",
        "            word = ' '.join(word) + ' </w>'\n",
        "            word_tokens = []\n",
        "            while word:\n",
        "                possible_tokens = [token for token in self.vocab if word.startswith(token.replace('</w>', ' </w>'))]\n",
        "                if not possible_tokens:\n",
        "                    word_tokens.append(word)\n",
        "                    break\n",
        "                longest_token = max(possible_tokens, key=len)\n",
        "                word_tokens.append(longest_token)\n",
        "                word = word[len(longest_token):].lstrip()\n",
        "            tokens.extend(word_tokens)\n",
        "        return tokens\n",
        "\n",
        "sentences = [\n",
        "    \"This darkness is absolutely killing! If we ever take this trip again, it must be about the time of the sNew Moon!\",\n",
        "    \"This is a tokenization task. Tokenization is the first step in a NLP pipeline. We will be comparing the tokens generated by each tokenization model.\"\n",
        "]\n",
        "\n",
        "processed_text = ' '.join(sentences).lower()\n",
        "processed_text = re.sub(r'[^a-z\\s]', '', processed_text)\n",
        "\n",
        "words = processed_text.split()\n",
        "word_freq = Counter(words)\n",
        "bpe_tokenizer = BPE(word_freq, vocab_size=500)\n",
        "tokenized_sentences = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    tokenized_sentence = bpe_tokenizer.tokenize(sentence)\n",
        "    tokenized_sentences.extend(tokenized_sentence)\n",
        "print(tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJL3bwbYK7-T",
        "outputId": "b2b0d38f-c977-4fa0-bc3b-06d8d6c80e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['t h i s </w>', 'd a r k n e s s </w>', 'i s </w>', 'a b s o l u t e l y </w>', 'k i l l i n g </w>', 'i f </w>', 'w e </w>', 'e v e r </w>', 't a k e </w>', 't h i s </w>', 't r i p </w>', 'a g a i n </w>', 'i t </w>', 'm u s t </w>', 'b e </w>', 'a b o u t </w>', 't h e </w>', 't i m e </w>', 'o f </w>', 't h e </w>', 's n e w </w>', 'm o o n </w>', 't h i s </w>', 'i s </w>', 'a</w>', '>', 't o k e n i z a t i o n </w>', 't a s k </w>', 't o k e n i z a t i o n </w>', 'i s </w>', 't h e </w>', 'f i r s t </w>', 's t e p </w>', 'i n </w>', 'a</w>', '>', 'n l p </w>', 'p i p e l i n e </w>', 'w e </w>', 'w i l l </w>', 'b e </w>', 'c o m p a r i n g </w>', 't h e </w>', 't o k e n s </w>', 'g e n e r a t e d </w>', 'b y </w>', 'e a c h </w>', 't o k e n i z a t i o n </w>', 'm o d e l </w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3_1\n"
      ],
      "metadata": {
        "id": "a0_piJhgdkHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "with open('/content/Tarzan.txt', 'r', encoding='utf-8') as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "text_data = re.sub(r'[^\\x00-\\x7F]+', ' ', text_data)\n",
        "text_data = text_data.lower()\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "text_data = text_data.translate(translator)\n",
        "\n",
        "tokens = text_data.split()\n",
        "\n",
        "from collections import Counter\n",
        "bigrams = zip(tokens[:-1], tokens[1:])\n",
        "\n",
        "bigram_counts = Counter(bigrams)\n",
        "bigram_counts.most_common(20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO9aLZlZdwPg",
        "outputId": "7738ad3b-8649-4b5d-b28a-c508d496fa2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('of', 'the'), 966),\n",
              " (('in', 'the'), 372),\n",
              " (('to', 'the'), 312),\n",
              " (('ibn', 'jad'), 227),\n",
              " (('and', 'the'), 190),\n",
              " (('upon', 'the'), 162),\n",
              " (('from', 'the'), 156),\n",
              " (('he', 'had'), 151),\n",
              " (('of', 'his'), 134),\n",
              " (('of', 'a'), 124),\n",
              " (('that', 'he'), 124),\n",
              " (('he', 'was'), 119),\n",
              " (('it', 'was'), 118),\n",
              " (('at', 'the'), 117),\n",
              " (('into', 'the'), 115),\n",
              " (('with', 'the'), 113),\n",
              " (('of', 'nimmr'), 109),\n",
              " (('the', 'great'), 99),\n",
              " (('by', 'the'), 98),\n",
              " (('for', 'the'), 96)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3_2"
      ],
      "metadata": {
        "id": "nhHYD-aaerVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import string\n",
        "\n",
        "def load_and_preprocess_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    text = text.lower()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def count_bigrams_and_tokens(tokens):\n",
        "    bigrams = zip(tokens[:-1], tokens[1:])\n",
        "    bigram_counts = Counter(bigrams)\n",
        "    token_counts = Counter(tokens)\n",
        "    return bigram_counts, token_counts\n",
        "\n",
        "def calculate_smoothed_probabilities(bigram_counts, token_counts):\n",
        "    vocabulary_size = len(set(token_counts))\n",
        "    smoothed_probabilities = {}\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        first_word_count = token_counts[bigram[0]]\n",
        "        smoothed_count = count + 1\n",
        "        smoothed_probability = smoothed_count / (first_word_count + vocabulary_size)\n",
        "        smoothed_probabilities[bigram] = smoothed_probability\n",
        "    return smoothed_probabilities\n",
        "\n",
        "def main(file_path):\n",
        "    tokens = load_and_preprocess_text(file_path)\n",
        "    bigram_counts, token_counts = count_bigrams_and_tokens(tokens)\n",
        "    smoothed_probabilities = calculate_smoothed_probabilities(bigram_counts, token_counts)\n",
        "    print(list(smoothed_probabilities.items())[:10])\n",
        "\n",
        "file_path = '/content/Tarzan.txt'\n",
        "main(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jb20TJkeqUy",
        "outputId": "fea7473f-ba94-410c-a437-19d0053fcfc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('the', 'project'), 0.0027414933075310434), (('project', 'gutenberg'), 0.012837155632482332), (('gutenberg', 'ebook'), 0.0005770340450086555), (('ebook', 'of'), 0.00029167274318214965), (('of', 'tarzan'), 0.002870508186264087), (('tarzan', 'lord'), 0.0015328874024526198), (('lord', 'of'), 0.0018895348837209302), (('of', 'the'), 0.10280671911545822), (('the', 'jungle'), 0.0077406869859700045), (('jungle', 'this'), 0.00028764562059542645)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3_3"
      ],
      "metadata": {
        "id": "mZinveyigE7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def preprocess_text(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        text = file.read().lower()\n",
        "        text = re.sub('[^a-z\\s]', '', text)\n",
        "    return text.split()\n",
        "\n",
        "def count_bigrams(words):\n",
        "    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
        "    return Counter(bigrams)\n",
        "\n",
        "def smoothed_probabilities(bigrams, vocabulary_size):\n",
        "    total_bigrams = sum(bigrams.values()) + vocabulary_size**2\n",
        "    probabilities = {bigram: (count + 1) / total_bigrams for bigram, count in bigrams.items()}\n",
        "    return probabilities\n",
        "\n",
        "def generate_text(start, probabilities, num_words=10):\n",
        "    current_word = start.split()[-1]\n",
        "    text = start\n",
        "    for _ in range(num_words):\n",
        "        next_words = [bigram[1] for bigram in probabilities if bigram[0] == current_word]\n",
        "        if not next_words:\n",
        "            break\n",
        "        current_word = next_words[0]\n",
        "        text += ' ' + current_word\n",
        "    return text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    filepath = '/content/Tarzan.txt'\n",
        "    words = preprocess_text(filepath)\n",
        "    bigrams = count_bigrams(words)\n",
        "    vocabulary_size = len(set(words))\n",
        "    probabilities = smoothed_probabilities(bigrams, vocabulary_size)\n",
        "\n",
        "    starting_phrases = [\n",
        "        \"knowing well the windings of the trail he\",\n",
        "        \"for half a day he lolled on the huge back and\"\n",
        "    ]\n",
        "\n",
        "    for phrase in starting_phrases:\n",
        "        print(generate_text(phrase, probabilities))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v14EdrzOgGnS",
        "outputId": "768748c7-9cf9-4523-df0c-e0adb16bcc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knowing well the windings of the trail he threw his great tourney the project gutenberg ebook of tarzan\n",
            "for half a day he lolled on the huge back and most other parts of tarzan lord of tarzan lord of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3_4\n",
        "Rewriting question 3_3, considering n = 3 and n = 5"
      ],
      "metadata": {
        "id": "GFxHHqi4iLdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def preprocess_text(filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        text = file.read().lower()\n",
        "        text = re.sub('[^a-z\\s]', '', text)\n",
        "    return text.split()\n",
        "\n",
        "def count_ngrams(words, n):\n",
        "    ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
        "    return Counter(ngrams)\n",
        "\n",
        "def smoothed_probabilities(ngrams, vocabulary_size):\n",
        "    total_ngrams = sum(ngrams.values()) + vocabulary_size ** n\n",
        "    probabilities = {ngram: (count + 1) / total_ngrams for ngram, count in ngrams.items()}\n",
        "    return probabilities\n",
        "\n",
        "def generate_text(start, probabilities, n, num_words=10):\n",
        "    current_words = tuple(start.split()[-(n-1):])\n",
        "    text = start\n",
        "    for _ in range(num_words):\n",
        "        next_words = [ngram[-1] for ngram in probabilities if ngram[:-1] == current_words]\n",
        "        if not next_words:\n",
        "            break\n",
        "        current_words = current_words[1:] + (next_words[0],)\n",
        "        text += ' ' + current_words[-1]\n",
        "    return text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    filepath = '/content/Tarzan.txt'\n",
        "    words = preprocess_text(filepath)\n",
        "\n",
        "    n_values = [3, 5]\n",
        "\n",
        "    for n in n_values:\n",
        "        ngrams = count_ngrams(words, n)\n",
        "        vocabulary_size = len(set(words))\n",
        "        probabilities = smoothed_probabilities(ngrams, vocabulary_size)\n",
        "\n",
        "       starting_phrases = [\n",
        "            \"knowing well the windings of the trail he\",\n",
        "            \"for half a day he lolled on the huge back and\"\n",
        "        ]\n",
        "\n",
        "        print(f\"Using {n}-gram:\")\n",
        "        for phrase in starting_phrases:\n",
        "            print(generate_text(phrase, probabilities, n))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5TWEkzYiQ2U",
        "outputId": "b48ab76e-5ada-4348-8b4e-5f0a47471673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 3-gram:\n",
            "knowing well the windings of the trail he did not hate mennot even white men with powerful express\n",
            "for half a day he lolled on the huge back and usha tore through the foliage swayed the giant bulk of\n",
            "Using 5-gram:\n",
            "knowing well the windings of the trail he took short cuts swinging through the branches of the trees\n",
            "for half a day he lolled on the huge back and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3_4\n",
        "Rewriting question 3_2, considering n = 3 and n = 5"
      ],
      "metadata": {
        "id": "miWHLbrXvdq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import string\n",
        "\n",
        "def load_and_preprocess_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
        "    text = text.lower()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "def count_ngrams_and_tokens(tokens, n):\n",
        "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
        "    ngram_counts = Counter(ngrams)\n",
        "    token_counts = Counter(tokens)\n",
        "    return ngram_counts, token_counts\n",
        "\n",
        "def calculate_smoothed_probabilities(ngram_counts, token_counts, n):\n",
        "    vocabulary_size = len(set(token_counts))\n",
        "    smoothed_probabilities = {}\n",
        "    for ngram, count in ngram_counts.items():\n",
        "        prefix = ngram[:-1]\n",
        "        prefix_count = token_counts[prefix]\n",
        "        smoothed_count = count + 1\n",
        "        smoothed_probability = smoothed_count / (prefix_count + vocabulary_size)\n",
        "        smoothed_probabilities[ngram] = smoothed_probability\n",
        "    return smoothed_probabilities\n",
        "\n",
        "def main(file_path, n):\n",
        "    tokens = load_and_preprocess_text(file_path)\n",
        "    ngram_counts, token_counts = count_ngrams_and_tokens(tokens, n)\n",
        "    smoothed_probabilities = calculate_smoothed_probabilities(ngram_counts, token_counts, n)\n",
        "    print(list(smoothed_probabilities.items())[:10])\n",
        "\n",
        "file_path = '/content/Tarzan.txt'\n",
        "n_values = [3, 5]\n",
        "\n",
        "for n in n_values:\n",
        "    print(f\"Using {n}-gram:\")\n",
        "    main(file_path, n)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sJG3cC8vmlc",
        "outputId": "4a751413-9007-444c-bd87-1f1ce6dff953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 3-gram:\n",
            "[(('the', 'project', 'gutenberg'), 0.004967855055523086), (('project', 'gutenberg', 'ebook'), 0.0005844535359438924), (('gutenberg', 'ebook', 'of'), 0.0002922267679719462), (('ebook', 'of', 'tarzan'), 0.0002922267679719462), (('of', 'tarzan', 'lord'), 0.0002922267679719462), (('tarzan', 'lord', 'of'), 0.0016072472238457044), (('lord', 'of', 'the'), 0.0018994739918176504), (('of', 'the', 'jungle'), 0.004967855055523086), (('the', 'jungle', 'this'), 0.0002922267679719462), (('jungle', 'this', 'ebook'), 0.0002922267679719462)]\n",
            "Using 5-gram:\n",
            "[(('the', 'project', 'gutenberg', 'ebook', 'of'), 0.0002922267679719462), (('project', 'gutenberg', 'ebook', 'of', 'tarzan'), 0.0002922267679719462), (('gutenberg', 'ebook', 'of', 'tarzan', 'lord'), 0.0002922267679719462), (('ebook', 'of', 'tarzan', 'lord', 'of'), 0.0002922267679719462), (('of', 'tarzan', 'lord', 'of', 'the'), 0.0002922267679719462), (('tarzan', 'lord', 'of', 'the', 'jungle'), 0.0016072472238457044), (('lord', 'of', 'the', 'jungle', 'this'), 0.0002922267679719462), (('of', 'the', 'jungle', 'this', 'ebook'), 0.0002922267679719462), (('the', 'jungle', 'this', 'ebook', 'is'), 0.0002922267679719462), (('jungle', 'this', 'ebook', 'is', 'for'), 0.0002922267679719462)]\n"
          ]
        }
      ]
    }
  ]
}